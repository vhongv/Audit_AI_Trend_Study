from openai import OpenAI
client = OpenAI()

embeddingt = client.embeddings.create(
    input="Your text goes here", model="text-embedding-3-small"
).data[0].embedding
len(embeddingt)
--------------------------------------------------------------------
import pandas as pd
import re

def remove_newlines(text):

  text = re.sub(r'\n',' ',text)
  text = re.sub(r' +',' ',text)
  return text

def text_to_df(data_file):
   texts=[]
   with open(data_file, 'r', encoding="utf-8") as file:
      text = file.read()
      sections = text.split('\n\n')
      for section in sections:
           lines = section.split('\n')
           fname = lines[0]
           content = ' '.join(lines[1:])
           texts.append([fname, content])
   df=pd.DataFrame(texts, columns=['fname','text'])
   df['text']=df['text'].apply(remove_newlines)
   return df

df=text_to_df('/content/drive/MyDrive/hotels.txt')
df.to_csv('sc.csv', index=False, encoding='utf-8')
---------------------------------------------------------------------------
pip install tiktoken
---------------------------------------------------------------------------
import pandas as pd
import tiktoken
from typing import List


embedding_model = "text-embedding-3-small"
embedding_encoding = "cl100k_base"
max_tokens = 1500

# 'scraped.csv' 파일을 불러와서 칼럼 이름을 'title'과 'text'로 변경
df = pd.read_csv("sc.csv")
df.columns = ['title', 'text']

tokenizer = tiktoken.get_encoding(embedding_encoding)
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))

def split_into_many (text, max_tokens = 500):

    # 텍스트를 문장별로 나누어 각 문장의 토큰 개수를 구함
    sentences = text.split('.')
    n_tokens = [len(tokenizer.encode(" " + sentence)) for sentence in sentences]

    chunks = []
    tokens_so_far = 0
    chunk = []

    # 각 문장과 토큰을 결합해 루프 처리
    for sentence, token in zip(sentences, n_tokens):

        # 지금까지의 토큰 수와 현재 문장의 토큰 수를 합한 값이
        # 최대 토큰 수를 초과하면 청크를 청크 목록에 추가하고
        # 청크 및 토큰 수를 재설정
        if tokens_so_far + token > max_tokens:
            chunks.append(". ".join(chunk) + ".")
            chunk = []
            tokens_so_far = 0

        # 현재 문장의 토큰 수가 최대 토큰 수보다 크면 다음 문장으로 넘어감
        if token > max_tokens:
            continue

        # 그렇지 않은 경우, 문장을 청크에 추가하고 토큰 수를 합계에 추가
        chunk.append(sentence)
        tokens_so_far += token + 1

    # 마지막 청크를 청크 목록에 추가
    if chunk:
        chunks.append(". ".join(chunk) + ".")
    return chunks

# 축약된 텍스트를 저장하기 위한 리스트
shortened = []

# DataFrame의 각 행에 대한 루프 처리
for row in df.iterrows():
    # 텍스트가 None인 경우 다음 줄로 넘어감
    if row[1]['text'] is None:
        continue
    # 토큰 수가 최대 토큰 수보다 큰 경우, 텍스트를
    # shortened 리스트에 추가
    if row[1]['n_tokens'] > max_tokens:
        shortened += split_into_many(row[1]['text'])

    # 그 외의 경우 텍스트를 그대로 'shortened' 목록에 추가
    else:
        shortened.append(row[1]['text'])

#"shortened"를 기반으로 새로운 DataFrame을 생성하고, 열 이름을 "text"로 지정
df = pd.DataFrame(shortened, columns = ['text'])

# 각 'text'의 토큰 수를 계산하여 새로운 열 'n_tokens'에 저장
df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))

def get_embedding(text, model="text-embedding-3-small"):
   text = text.replace("\n", " ")
   return client.embeddings.create(input = [text], model=model).data[0].embedding

df['embeddings'] = df.text.apply(lambda x: get_embedding(x, model=embedding_model))
df.to_csv('embeds.csv', index=False)
---------------------------------------------------------------------------------------------
def create_context(question, df, max_len=1800):
    """
        질문과 학습 데이터를 비교해 컨텍스트를 만드는 함수
    """

    # 질문을 벡터화
    q_embeddings = client.embeddings.create(input=[question], model='text-embedding-3-small').data[0].embedding

    # 질문과 학습 데이터와 비교하여 코사인 유사도를 계산하고
    # 'distances' 열에 유사도를 저장
    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].apply(eval).apply(np.array).values, distance_metric='cosine')

    # 컨텍스트를 저장하기 위한 리스트
    returns = []
    # 컨텍스트의 현재 길이
    cur_len = 0

    # 학습 데이터를 유사도 순으로 정렬하고 토큰 개수 한도까지 컨텍스트에
    # 추가
    for _, row in df.sort_values('distances', ascending=True).iterrows():
        # 텍스트 길이를 현재 길이에 더하기
        cur_len += row['n_tokens'] + 4

        # 텍스트가 너무 길면 루프 종료
        if cur_len > max_len:
            break

        # 컨텍스트 목록에 텍스트 추가하기
        returns.append(row["text"])

    # 컨텍스트를 결합해 반환
    return "\n\n###\n\n".join(returns)
-------------------------------------------------------------------------------------------------
def answer_question(question, conversation_history):
    """
    문맥에 따라 질문에 답하는 기능
    """

    # 학습 데이터 불러오기
    df = pd.read_csv('embeds.csv')

    context = create_context(question, df, max_len=200)  #←질문과 학습 데이터를 비교해 컨텍스트 생성
    # 프롬프트를 생성하고 대화 기록에 추가하기
    prompt = f"당신은 어느 호텔 직원입니다. 문맥에 따라 고객의 질문에 정중하게 대답해 주십시오. 컨텍스트가 질문에 대답할 수 없는 경우 '모르겠습니다'라고 대답하세요.\n\n컨텍스트: {context}\n\n---\n\n질문: {question}\n답변:"
    conversation_history.append({"role": "user", "content": prompt})

    try:
        # ChatGPT에서 답변 생성
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=conversation_history,
            temperature=1,
        )

        # ChatGPT에서 답변 반환
        return response.choices[0].message.content.strip()
    except Exception as e:
        # 오류가 발생하면 빈 문자열을 반환
        print(e)
        return ""
----------------------------------------------------------------------------------------------------
def distances_from_embeddings(
    query_embedding: List[float],
    embeddings: List[List[float]],
    distance_metric="cosine",
) -> List[List]:
    """Return the distances between a query embedding and a list of embeddings."""
    distance_metrics = {
        "cosine": spatial.distance.cosine,
        "L1": spatial.distance.cityblock,
        "L2": spatial.distance.euclidean,
        "Linf": spatial.distance.chebyshev,
    }
    distances = [
        distance_metrics[distance_metric](query_embedding, embedding)
        for embedding in embeddings
    ]
    return distances
-----------------------------------------------------------------------------------------------------------
conversation_history = []
while True:
    # 사용자가 입력한 문자를 'user_input' 변수에 저장
    user_input = input("User : ")

    # 사용자가 입력한 문자가 'exit'인 경우 루프에서 빠져나옴
    if user_input == "exit":
        break

    conversation_history.append({"role": "user", "content": user_input})
    answer = answer_question(user_input, conversation_history)

    print(f"ChatGPT: {answer}")
    conversation_history.append({"role": "assistant", "content":answer})

