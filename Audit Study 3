!pip install torch transformers

######################################################################################################################
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# GPT 모델과 토크나이저 불러오기
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

def generate_response(prompt, max_length=50):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")

    # 모델을 사용하여 응답 생성
    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)

    # 생성된 응답 디코딩
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

def main():
    print("채팅을 시작하세요. 종료하려면 '종료'를 입력하세요.")
    while True:
        user_input = input("사용자: ")
        if user_input == "종료":
            print("채팅을 종료합니다. 안녕히 가세요!")
            break
        else:
            bot_response = generate_response(user_input)
            print("챗봇:", bot_response)

if __name__ == "__main__":
    main()

################################################################################################################
import yfinance as yf

# 삼성전자 주식 코드
stock_code = "005930.KS"

# 데이터 불러오기
samsung_data = yf.download(stock_code, start="2012-01-01", end="2024-02-08")

# CSV 파일로 저장
samsung_data.to_csv("samsung_stock_prices.csv")

#####################################################################################################
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense

# Load dataset
data = pd.read_csv("samsung_stock_prices.csv")
prices = data['Close'].values.reshape(-1, 1)

# Normalize data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_prices = scaler.fit_transform(prices)

# Function to create dataset
def create_dataset(data, look_back=1):
    X, Y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:(i + look_back), 0])
        Y.append(data[i + look_back, 0])
    return np.array(X), np.array(Y)

# Create train and test datasets
train_size = int(len(scaled_prices) * 0.8)
test_size = len(scaled_prices) - train_size
train_data, test_data = scaled_prices[0:train_size, :], scaled_prices[train_size:len(scaled_prices), :]

# Reshape into X=t and Y=t+1
look_back = 1
train_X, train_Y = create_dataset(train_data, look_back)
test_X, test_Y = create_dataset(test_data, look_back)

# Reshape input to be [samples, time steps, features]
train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))
test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))

# Create LSTM model
model = Sequential()
model.add(LSTM(units=50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(units=1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(train_X, train_Y, epochs=10, batch_size=1, verbose=2)

# Make predictions
train_predictions = model.predict(train_X)
test_predictions = model.predict(test_X)

# Inverse transform predictions
train_predictions = scaler.inverse_transform(train_predictions)
train_Y = scaler.inverse_transform([train_Y])
test_predictions = scaler.inverse_transform(test_predictions)
test_Y = scaler.inverse_transform([test_Y])

# Plot predictions
plt.figure(figsize=(13,8))
plt.plot(train_Y.flatten(), label='Actual Train Prices')
plt.plot(train_predictions.flatten(), label='Predicted Train Prices')
plt.plot(test_Y.flatten(), label='Actual Test Prices')
plt.plot(test_predictions.flatten(), label='Predicted Test Prices')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.title('Stock Price Prediction using LSTM')
plt.legend()
plt.show()

print(test_predictions[-1])

##################################################################
import numpy as np

# 시그모이드 활성화 함수
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# tanh 활성화 함수
def tanh(x):
    return np.tanh(x)

# LSTM cell 클래스
class LSTMCell:
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size

        # 가중치 초기화
        self.Wf = np.random.randn(input_size + hidden_size, hidden_size)
        self.Wi = np.random.randn(input_size + hidden_size, hidden_size)
        self.Wc = np.random.randn(input_size + hidden_size, hidden_size)
        self.Wo = np.random.randn(input_size + hidden_size, hidden_size)
        self.bf = np.zeros((1, hidden_size))
        self.bi = np.zeros((1, hidden_size))
        self.bc = np.zeros((1, hidden_size))
        self.bo = np.zeros((1, hidden_size))

    def forward(self, x, h_prev, c_prev):
        # 입력과 이전 은닉 상태를 연결
        concat = np.concatenate((x, h_prev), axis=1)

        # forget gate
        f = sigmoid(np.dot(concat, self.Wf) + self.bf)
        
        # input gate
        i = sigmoid(np.dot(concat, self.Wi) + self.bi)
        
        # cell state (candidate)
        c_hat = tanh(np.dot(concat, self.Wc) + self.bc)
        
        # update cell state
        c = f * c_prev + i * c_hat
        
        # output gate
        o = sigmoid(np.dot(concat, self.Wo) + self.bo)
        
        # hidden state
        h = o * tanh(c)
        
        return h, c

# 입력 데이터 생성
X = np.array([[0.1, 0.2, 0.3],
              [0.2, 0.3, 0.4],
              [0.3, 0.4, 0.5]])

input_size = 3
hidden_size = 5

# LSTM 셀 생성
lstm_cell = LSTMCell(input_size, hidden_size)

# 초기 은닉 상태와 셀 상태 설정
h_prev = np.zeros((1, hidden_size))
c_prev = np.zeros((1, hidden_size))

# 각 시간 단계에 대한 순방향 전파
for t in range(X.shape[0]):
    x_t = X[t].reshape(1, -1)
    h_prev, c_prev = lstm_cell.forward(x_t, h_prev, c_prev)
    print("Output at time", t, ":", h_prev)


##############################################################################################
import numpy as np

class SimpleRNN:
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # 입력에서 은닉층으로의 가중치 초기화
        self.W_xh = np.random.randn(hidden_size, input_size)
        # 은닉층에서 은닉층으로의 가중치 초기화
        self.W_hh = np.random.randn(hidden_size, hidden_size)
        # 은닉층에서 출력층으로의 가중치 초기화
        self.W_hy = np.random.randn(input_size, hidden_size)
        
        # 편향 초기화
        self.b_h = np.zeros((hidden_size, 1))
        self.b_y = np.zeros((input_size, 1))
        
    def forward(self, x, h_prev):
        # 현재 입력과 이전 상태를 받아 새로운 상태를 계산합니다.
        
        # 은닉층의 활성화 함수는 tanh를 사용합니다.
        h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)
        # 출력층의 활성화 함수는 softmax를 사용합니다.
        y = np.dot(self.W_hy, h) + self.b_y
        
        # 새로운 상태와 출력값 반환
        return h, y

# 입력 데이터와 이전 상태 초기화
input_size = 3
hidden_size = 2
x = np.array([[0.1], [0.2], [0.3]])  # 입력 데이터
h_prev = np.zeros((hidden_size, 1))  # 초기 은닉 상태

# SimpleRNN 객체 생성
rnn = SimpleRNN(input_size, hidden_size)

# 순방향 전파 실행
h_next, y = rnn.forward(x, h_prev)

# 결과 출력
# SimpleRNN 객체 생성
rnn = SimpleRNN(input_size, hidden_size)

# 순방향 전파 실행
h_next, y = rnn.forward(x, h_prev)

# 결과 출력
print("새로운 상태 h:", h_next.reshape(1,-1))
print("출력값 y:", y.reshape(1,-1))



